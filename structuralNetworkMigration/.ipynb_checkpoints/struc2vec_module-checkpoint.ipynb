{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# struc2vec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from struc2vec.src.PreProcess import *\n",
    "from struc2vec.src.GraphAlgorithms import GraphAlgorithms as ga\n",
    "from struc2vec.src.RandomWalker import *\n",
    "import networkx as nx\n",
    "\n",
    "class struc2vec():\n",
    "    def __init__(self, G, preprocess=True):\n",
    "        '''\n",
    "        First a graph object is created, this stores the relevant information of the graph and allows the application\n",
    "        of the struc2vec algorithm and storing the results.\n",
    "\n",
    "        If graph is directed an undirected graph is made for estimating diameter and for getting neighborhoods in for context graph.\n",
    "\n",
    "        The graph is set to be preprocess by default, if the context graph has already been generated and needs only to be loaded, it should be set to false.\n",
    "        '''\n",
    "        self.is_directed = nx.is_directed(G)\n",
    "        self.G_UD = nx.Graph(G)\n",
    "        if self.is_directed:\n",
    "            self.G_D = G\n",
    "        self.nodes = [*self.G_UD.nodes] # List of all nodes\n",
    "        self.nodePairs = self.getNodePairs()\n",
    "        self.diameter = nx.diameter(self.G_UD)\n",
    "        if preprocess:\n",
    "            self.DegreeSequences = getDegreeSequences(self.G_UD, self.G_D)\n",
    "    \n",
    "    def getNodePairs(self):\n",
    "        # Get all unique node pairs as tuples ignoring order\n",
    "        nodePairs = []\n",
    "        for i, v in enumerate(self.nodes[:-1]):\n",
    "            for d in self.nodes[i+1:]:\n",
    "                nodePairs.append((v,d))\n",
    "        return nodePairs\n",
    "    \n",
    "    def getMultiLevelGraph(self, path=None):\n",
    "        \"\"\"\n",
    "        This function generates and saves the context graph for the random walks. It has no output, but saves the G_ML for the object.\n",
    "\n",
    "        If the context graph has been created, it takes the path as input, and loads and stores the G_ML in the object instead.\n",
    "        \"\"\"\n",
    "        if path:\n",
    "            if path[-1] != \"/\":\n",
    "                path = path + \"/\"\n",
    "        self.G_ML, self.adj_dicts = ga().MultiLevelGraph(self,self.diameter,path=path)\n",
    "        self.n_layers = self.diameter\n",
    "        self.upweightdict = ga().getUpWeightDict(self.G_ML)\n",
    "\n",
    "    def getRandomWalks(self, start_node=None, number_of_walks=100, walk_length=10, q=0.2):\n",
    "        walks = random_walk(self,start_node, number_of_walks, walk_length, q)\n",
    "        return walks\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphAlgrorithm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import fastdtw\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "class GraphAlgorithms():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        This object contains the different functions to be applied on the graph object for the struc2vec algorithm.\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    # def getDegreeSequence(self, s2vG, origin, n_steps):\n",
    "    #     '''\n",
    "    #     The function returns a sorted degree sequence for a given node based on \n",
    "    #     n-step neighbors.\n",
    "    #     '''\n",
    "    #     visitedNodes = {}\n",
    "    #     que = [origin]\n",
    "    #     for _ in range(n_steps):\n",
    "    #         newQue = []\n",
    "    #         for node_0 in que:\n",
    "    #             if node_0 not in visitedNodes:\n",
    "    #                 visitedNodes[node_0] = len(s2vG.G[node_0])\n",
    "    #             for node_1 in s2vG.G[node_0]:\n",
    "    #                 if node_1 not in visitedNodes:\n",
    "    #                     newQue.append(node_1)\n",
    "    #         que = newQue\n",
    "    #     degreeSequence = sorted(visitedNodes.values(), reverse=True)\n",
    "    #     return degreeSequence\n",
    "    \n",
    "    # def getDSlayer(self, s2vG, n_steps):\n",
    "    #     '''\n",
    "    #     For a given layer, get the degree sequence for all nodes\n",
    "    #     '''\n",
    "    #     nodeDS = {} # Container for Degree sequences\n",
    "    #     for v in s2vG.nodes:\n",
    "    #         # Calculate degree sequence and store it indexed by node\n",
    "    #         DS = self.getDegreeSequence(s2vG, v, n_steps)\n",
    "    #         nodeDS[v] = DS\n",
    "    #     return nodeDS\n",
    "    \n",
    "    # def calcDTW(self, s2vG, n_steps):\n",
    "    #     '''\n",
    "    #     Calculate DTW distance between DS for each node in nodepairs and return the distance between all nodepairs\n",
    "    #     '''\n",
    "    #     # For the calculation, we first need to get the degree sequence for each node\n",
    "    #     nodeDS = self.getDSlayer(s2vG, n_steps)\n",
    "        \n",
    "    #     ssm_nodepairs = {}\n",
    "    #     for v0, v1 in s2vG.nodePairs:\n",
    "    #         # Each DS is converted to an numpy array and reshaped into 1-D arrays (or rather (n,1) \n",
    "    #         # where n is the length of the array)\n",
    "    #         ds0 = np.array(nodeDS[v0])\n",
    "    #         ds0 = ds0.reshape(len(ds0),1) \n",
    "    #         ds1 = np.array(nodeDS[v1])\n",
    "    #         ds1 = ds1.reshape(len(ds1),1)\n",
    "\n",
    "    #         distance, path = fastdtw.fastdtw(ds0, ds1, dist=self.d_func)\n",
    "    #         ssm_nodepairs[(v0,v1)] = distance\n",
    "    #     return ssm_nodepairs\n",
    "\n",
    "    def d_func(self,a,b):\n",
    "        '''\n",
    "        Calculate distance\n",
    "        '''\n",
    "        return float((max(a,b)/min(a,b))-1)\n",
    "    \n",
    "    def calculateDistances(self, s2vG, n_steps):\n",
    "        \"\"\"\n",
    "        The function utilizes the preprocessed degreesequences and calculates a distance\n",
    "        measure for the in- and out-degree sequences between all node pairs.\n",
    "\n",
    "        The final distance measure is the average of the in- and out-distances.\n",
    "\n",
    "        The output is a dictionary with a tuple of the nodepairs as key and distance as value\n",
    "        \"\"\"\n",
    "        edgelist_dist = []\n",
    "        for v0, v1 in s2vG.nodePairs:\n",
    "            arr0_in = np.array(sorted(s2vG.DegreeSequences[v0][n_steps][\"in\"], reverse=True))\n",
    "            arr0_out = np.array(sorted(s2vG.DegreeSequences[v0][n_steps][\"out\"], reverse=True))\n",
    "            arr1_in = np.array(sorted(s2vG.DegreeSequences[v1][n_steps][\"in\"], reverse=True))\n",
    "            arr1_out = np.array(sorted(s2vG.DegreeSequences[v1][n_steps][\"out\"], reverse=True))\n",
    "\n",
    "            arr0_in = arr0_in.reshape(len(arr0_in),1)\n",
    "            arr1_in = arr1_in.reshape(len(arr1_in),1)\n",
    "            arr0_out = arr0_out.reshape(len(arr0_out),1)\n",
    "            arr1_out = arr1_out.reshape(len(arr1_out),1)\n",
    "\n",
    "            arr0_in = arr0_in + np.ones((len(arr0_in),1))\n",
    "            arr1_in = arr1_in + np.ones((len(arr1_in),1))\n",
    "            arr0_out = arr0_out + np.ones((len(arr0_out),1))\n",
    "            arr1_out = arr1_out + np.ones((len(arr1_out),1))\n",
    "\n",
    "            dist_in, conv_vect = fastdtw.fastdtw(arr0_in, arr1_in, dist=self.d_func)\n",
    "            dist_out, conv_vect = fastdtw.fastdtw(arr0_out, arr1_out, dist=self.d_func)\n",
    "\n",
    "            prev_weight = 0\n",
    "            if n_steps != 0:\n",
    "                prev_weight = self.G_ML[n_steps-1].edges[v0,v1][\"weight\"]\n",
    "            edge = (v0,v1,{\"weight\": prev_weight + np.exp(-np.mean([dist_in, dist_out]))})\n",
    "            edgelist_dist.append(edge)\n",
    "        return edgelist_dist\n",
    "\n",
    "    # def getStrucGraph(self, s2vG, n_steps):\n",
    "    #     '''\n",
    "    #     This function will calculate the context graph for a specified layer in graph generated from structural similarity.\n",
    "\n",
    "    #     It takes s2vG-object as input and the number of layers of neighbors to include for comparison of each node pair.\n",
    "    #     '''\n",
    "    #     G = nx.Graph()\n",
    "        \n",
    "    #     # To generate the graph we need to calculate the similarity scores for all nodepairs\n",
    "    #     # To do this, we will calculate the DTW distance based on similarity measures of each node\n",
    "    #     dist_nodepairs = self.calculateDistances(s2vG, n_steps)\n",
    "        \n",
    "    #     # Now all the edges are added and the weight is calculated\n",
    "    #     for (v0,v1), dist in dist_nodepairs.items():\n",
    "    #         G.add_edge(v0,v1,weight=dist)\n",
    "    #     return G\n",
    "    \n",
    "    def MultiLevelGraph(self, s2vG, n_level, path=None):\n",
    "        \"\"\"\n",
    "        This function takes a s2vG-object as input and generates the responding context graph for it. If a path is defined, it loads the graph otherwise the graph is \n",
    "        created. It outputs the context graph as a dictionary object, with layers as keys and graphs as values.\n",
    "        \"\"\"\n",
    "        self.G_ML = {}\n",
    "        if path:\n",
    "            for file in os.listdir(path):\n",
    "                layer_n = int(file.split(\".\")[0])\n",
    "                graph = nx.read_gexf(path + file)\n",
    "                self.G_ML[layer_n] = graph\n",
    "        else:\n",
    "            for i in range(n_level+1):\n",
    "                edgelist_i = self.calculateDistances(s2vG, i)\n",
    "                G_i = nx.Graph(edgelist_i)\n",
    "                self.G_ML[i] = G_i\n",
    "\n",
    "        adj_dicts = self.getAdjacencyDicts(self.G_ML)\n",
    "        return self.G_ML, adj_dicts\n",
    "\n",
    "    \n",
    "    def getAdjacencyDicts(self, G_ML):\n",
    "        adj_dicts = {}\n",
    "        for layer, graph in G_ML.items():\n",
    "            adj_dicts[layer] = {}\n",
    "            for node, neighbors in graph.adjacency():\n",
    "                adj_dicts[layer][node] = {neighbor:attr[\"weight\"] for neighbor, attr in neighbors.items()}\n",
    "        return adj_dicts\n",
    "    \n",
    "    def getUpWeightsLayer(self, G):\n",
    "        '''\n",
    "        The function takes a graph as input, and calculates the edge weight for all node-node edges from layer n \n",
    "        to layer n+1.\n",
    "        It returns a dictionary with nodes as keys and weight as value.\n",
    "        '''\n",
    "        # Calculate mean weight of all edges in graph\n",
    "        avg_edge_weight = np.mean([G.edges[edge][\"weight\"] for edge in G.edges])\n",
    "\n",
    "        # Dict for edges\n",
    "        upEdge = {}\n",
    "\n",
    "        # Get weight for each node\n",
    "        for node in G.nodes:\n",
    "            aboveAvgEdgeWeight = np.mean([int(att[\"weight\"] >= avg_edge_weight) for edge, att in G[node].items()])\n",
    "            upWeight = np.log(aboveAvgEdgeWeight + np.exp(1))\n",
    "            upEdge[node] = upWeight / (upWeight+1)\n",
    "\n",
    "        return upEdge\n",
    "\n",
    "\n",
    "    def getUpWeightDict(self, G_multilayer):\n",
    "        '''\n",
    "        The function takes a dictionary for a multilayer graph as input, and returns a \n",
    "        dictionary of all node-node edge weights for each layer in the multilayer graph.\n",
    "        The function is an iterator, utilizing the function getUpWeightsLayer to retrieve weights for each layer\n",
    "        '''\n",
    "        upWeightDict = {}\n",
    "        for layer, G in G_multilayer.items():\n",
    "            upWeightsLayer = self.getUpWeightsLayer(G)\n",
    "            upWeightDict[layer] = upWeightsLayer\n",
    "        return upWeightDict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "def scale_weights(weights, min_scale=0.1, max_scale=10):\n",
    "    \"\"\"\n",
    "    The functions takes a list of weights as input and rescales the values to \n",
    "    the set range min_scale-max_scale\n",
    "    \"\"\"\n",
    "    min_weight = np.min(weights)\n",
    "    max_weight = np.max(weights)\n",
    "    scale = lambda x: (((max_scale-min_scale)*(x-min_weight))/(max_weight-min_weight))+min_scale\n",
    "    return [scale(x) for x in weights]\n",
    "\n",
    "def get_meta_data(G, nodes, embeddings, cmap=None):\n",
    "    df_meta = pd.DataFrame([(node, embedding) for node,embedding in zip(nodes,embeddings)], columns=[\"Node\", \"Embedding\"])\n",
    "\n",
    "    # Clustering\n",
    "    clustering = nx.clustering(G)\n",
    "    df_meta[\"Clustering\"] = [clustering[node] for node in df_meta[\"Node\"]]\n",
    "\n",
    "    # Betweenness centrality\n",
    "    betweenness = nx.betweenness_centrality(G)\n",
    "    df_meta[\"Betweenness_centrality\"] = [betweenness[node] for node in df_meta[\"Node\"]]\n",
    "\n",
    "    # Closeness centrality\n",
    "    closeness = nx.closeness_centrality(G)\n",
    "    df_meta[\"Closeness_centrality\"] = [closeness[node] for node in df_meta[\"Node\"]]\n",
    "\n",
    "    # In- and out-degree\n",
    "    A = nx.adjacency_matrix(G, weight=None) # All cells are either 0 or 1 to support counting for degree\n",
    "    ind2node = {i:node for i, node in enumerate(G.nodes())}\n",
    "    # From the adj matrix, the in and out degree can be calculated as a sum of each row\n",
    "    # and corresponding column\n",
    "    degreeDict = {}\n",
    "    for i in range(A.shape[0]):\n",
    "        inDegree = A[:,[i]].sum()\n",
    "        outDegree = A[[i],:].sum()\n",
    "        degreeDict[ind2node[i]] = {\"inDegree\":inDegree, \"outDegree\":outDegree}\n",
    "    df_meta[\"InDegree\"] = [degreeDict[node][\"inDegree\"] for node in df_meta[\"Node\"]]\n",
    "    df_meta[\"OutDegree\"] = [degreeDict[node][\"outDegree\"] for node in df_meta[\"Node\"]]\n",
    "\n",
    "    # Average weight (currently for bot in- and out-edges)\n",
    "    avg_weight = {}\n",
    "    for node in G.nodes():\n",
    "        data = G[node]\n",
    "        weights = []\n",
    "        for dest, data in data.items():\n",
    "            weights.append(data[\"weight\"])\n",
    "        avg_weight[node] = np.mean(weights)\n",
    "    df_meta[\"Average_weight\"] = [avg_weight[node] for node in df_meta[\"Node\"]]\n",
    "\n",
    "    # Groupby embeddings\n",
    "    df_grouped = pd.DataFrame(df_meta.groupby(\"Embedding\")[\"Node\"].count())\n",
    "    df_grouped.columns = [\"Number of nodes\"]\n",
    "\n",
    "    if cmap:\n",
    "        df_grouped[\"Color\"] = [cmap[emb] for emb in df_grouped.index]\n",
    "    \n",
    "    avg_data = df_meta.drop(\"Node\",axis=1).groupby(\"Embedding\").mean().apply(lambda x: round(x, 3))\n",
    "\n",
    "    return df_meta, df_grouped.merge(avg_data, right_index=True, left_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def getDegreeSequences(G_UD, G_D):\n",
    "    \"\"\"\n",
    "    The function takes an undirected and a directed network as input (from the same graph).\n",
    "\n",
    "    It calculates the in- and out-degree sequences for each node for all layers of\n",
    "    a the nodes neighborhoods\n",
    "\n",
    "    The final output is a dictionary in the style {origin_node: {0:self, 1:degree_sequence 0 extended with \n",
    "    degree sequence from all immidiate neighbors, 2: degree_sequence 1 extended with degrees for 2-step neighbors}}\n",
    "    \"\"\"\n",
    "    # 0\n",
    "    A = nx.adjacency_matrix(G_D, weight=None) # All cells are either 0 or 1 to support counting for degree\n",
    "    ind2node = {i:node for i, node in enumerate(G_D.nodes())}\n",
    "    diameter = nx.diameter(G_UD)\n",
    "    \n",
    "    # 1\n",
    "    # From the adj matrix, the in and out degree can be calculated as a sum of each row\n",
    "    # and corresponding column\n",
    "    degreeDict = {}\n",
    "    for i in range(A.shape[0]):\n",
    "        inDegree = A[:,[i]].sum()\n",
    "        outDegree = A[[i],:].sum()\n",
    "        degreeDict[ind2node[i]] = {\"inDegree\":inDegree, \"outDegree\":outDegree}\n",
    "\n",
    "    # 2\n",
    "    # For all node pairs the length of the shortest path are estimated, this will\n",
    "    # then be used for generating the layers of neighborhoods by 'reversing' the dictionary\n",
    "    all_shortest_paths = {node:paths for node, paths in nx.all_pairs_shortest_path_length(G_UD)}\n",
    "\n",
    "    neighborhood_by_layer = {}\n",
    "    for origin_node, neighbors in all_shortest_paths.items():\n",
    "        neighborhood_by_layer[origin_node] = {}\n",
    "        for node, layer in neighbors.items():\n",
    "            if layer not in neighborhood_by_layer[origin_node]:\n",
    "                neighborhood_by_layer[origin_node][layer] = []\n",
    "            neighborhood_by_layer[origin_node][layer].append(node)\n",
    "\n",
    "    # 3\n",
    "    # Now each vector of nodes generated in #2 are converted to their corresponding in-\n",
    "    # and out-degree which was calculated in #1.\n",
    "    # Each degree sequence is aggregated from the current layer and all previous layers.\n",
    "    degree_vectors = {}\n",
    "\n",
    "    for origin_node, neighborhoods in neighborhood_by_layer.items():\n",
    "        degree_vectors[origin_node] = {}\n",
    "        for layer in range(diameter+1):\n",
    "            # DEL: [If all nodes are already included, the vec-in and vec_out are not recalculated but are added for all further layers]\n",
    "            if layer in neighborhoods.keys():\n",
    "                nodes = neighborhoods[layer]\n",
    "                vector_in = [degreeDict[node][\"inDegree\"] for node in nodes]\n",
    "                vector_out = [degreeDict[node][\"outDegree\"] for node in nodes]\n",
    "                degree_vectors[origin_node][layer] = {\"in\": vector_in, \"out\":vector_out}\n",
    "            else:\n",
    "                degree_vectors[origin_node][layer] = {\"in\": [1], \"out\":[1]}\n",
    "            #     if layer == 0:\n",
    "            #         degree_vectors[origin_node][layer] = {\"in\": vector_in, \"out\":vector_out}\n",
    "            #         continue\n",
    "            #     else:\n",
    "            #         vec_in_prev = degree_vectors[origin_node][layer-1][\"in\"].copy()\n",
    "            #         vec_in_prev.extend(vector_in)\n",
    "\n",
    "            #         vec_out_prev = degree_vectors[origin_node][layer-1][\"out\"].copy()\n",
    "            #         vec_out_prev.extend(vector_out)\n",
    "\n",
    "            # degree_vectors[origin_node][layer] = {}\n",
    "            # degree_vectors[origin_node][layer][\"in\"] = vec_in_prev\n",
    "            # degree_vectors[origin_node][layer][\"out\"] = vec_out_prev\n",
    "\n",
    "    # 4\n",
    "    # Finally return the dictionary\n",
    "    return degree_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomWalker.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def random_walk_step(current_node, s2vG, n_layer):\n",
    "    '''\n",
    "    The function picks a random node from the current nodes neighbors based on probabilities calculated from adjacency dict\n",
    "    '''\n",
    "    adj_dict = s2vG.adj_dicts[n_layer]\n",
    "    weight_total = sum(adj_dict[current_node].values())\n",
    "    neigh = list(adj_dict[current_node].keys())\n",
    "    prob = [w / weight_total for w in adj_dict[current_node].values()]\n",
    "    return np.random.choice(neigh, size=1, p=prob)[0]\n",
    "\n",
    "def random_walk(s2vG, start_node, number_of_walks, walk_length, q):\n",
    "    # Start at the bottom layer\n",
    "    n_layer = 0\n",
    "    walks = []\n",
    "    for i in range(number_of_walks):\n",
    "        current_node = start_node if start_node else np.random.choice(s2vG.nodes)\n",
    "        walk = [current_node]\n",
    "        for j in range(walk_length):\n",
    "            n_layer = getLayer(s2vG, n_layer, current_node, q)\n",
    "            current_node = random_walk_step(current_node, s2vG, n_layer)\n",
    "            walk.append(current_node)\n",
    "        walks.append(walk)\n",
    "    return walks\n",
    "\n",
    "def getLayer(s2vG, n_layer, current_node, q):\n",
    "    if np.random.random() > q:\n",
    "        return n_layer\n",
    "    else:\n",
    "        up_p = s2vG.upweightdict[n_layer][current_node]\n",
    "        if n_layer == 0:\n",
    "            n_layer = 1\n",
    "        elif n_layer == s2vG.n_layers:\n",
    "            n_layer = n_layer - 1\n",
    "        elif (np.random.random() >= up_p):\n",
    "            n_layer = n_layer - 1\n",
    "        else:\n",
    "            n_layer = n_layer + 1\n",
    "        return n_layer\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
